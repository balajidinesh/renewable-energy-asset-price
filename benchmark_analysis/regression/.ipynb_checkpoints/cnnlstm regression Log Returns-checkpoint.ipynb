{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "128e0c68-301d-4d0c-a90f-6951f31a70fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ebbbfe8-5f9c-4e6a-a74d-b825824a84e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from scipy.special import expit\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle as sklearn_shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, LSTM, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# from misc_modules import dm_test, plot_double_standard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f37ca-5b1f-409c-b2af-0a375ad37898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72d41fe4-d030-459a-a604-695839bfc284",
   "metadata": {},
   "outputs": [],
   "source": [
    "etfs = ['ICLN', 'PBD', 'QCLN']  # ETF symbols\n",
    "sequence_size = 4  # Number of time steps in sequence\n",
    "cross_window = 3  # Number of cross-validation windows\n",
    "\n",
    "lags = [1]\n",
    "predType = 'ahead_Return'\n",
    "predLabel = 'Log-Return'\n",
    "\n",
    "pred_size=250\n",
    "model_name = f'reg-{predType}'\n",
    "\n",
    "save_path = '../../results'\n",
    "data_path = '../../data'\n",
    "# 'GT Sent', 'INV Sent', 'GT_VAL_SENT', 'INV_VAL_SENT',\n",
    "sent_dict =  {\n",
    "    'SENT': [ 'log_ovx', 'log_return', 'log_navR', 'GT_VAL_SENT', 'INV_VAL_SENT', 'd1-inv','d2-gt'],\n",
    "    'NO_SENT': [ 'log_ovx', 'log_return', 'log_navR']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0beab59-331f-4a68-aea8-56b34bbf5d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_loss(y_true,y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    return tf.keras.losses.Huber(delta=delta)(y_true, y_pred)\n",
    "\n",
    "def mean_squared_log_error(y_true, y_pred):\n",
    "    return tf.keras.losses.MeanSquaredLogarithmicError()(y_true, y_pred)\n",
    "\n",
    "\n",
    "class model:\n",
    "    def __init__(self, model_type, input_shape=None):\n",
    "        self.model_type = model_type\n",
    "        self.input_shape = input_shape\n",
    "        self.model = self._choose_model()\n",
    "\n",
    "    def _choose_model(self):\n",
    "        \"\"\"Choose and instantiate the model based on the provided model type.\"\"\"\n",
    "        if self.model_type == 'cnnlstm':\n",
    "            if self.input_shape is None:\n",
    "                raise ValueError(\"input_shape must be provided for cnnlstm model.\")\n",
    "            return self._build_cnn_lstm_model()\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid model type: {self.model_type}\")\n",
    "\n",
    "    def _build_cnn_lstm_model(self):\n",
    "        \"\"\"Build the CNN-LSTM model for time series or sequence data.\"\"\"\n",
    "        Timesteps, No_Features = self.input_shape\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(filters=32, kernel_size=1, activation='relu', input_shape=(Timesteps, No_Features)))\n",
    "        model.add(MaxPooling1D(pool_size=1))\n",
    "        model.add(LSTM(32, activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(8))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer='adam', loss=huber_loss, metrics=['mae'])\n",
    "        return model\n",
    "\n",
    "    def fit(self, X_train, y_train,epochs=100, batch_size=32, validation_split=0.2,shuffle=True):\n",
    "        \"\"\"Fit the model depending on the type.\"\"\"\n",
    "\n",
    "        \n",
    "        if self.model_type in ['svr', 'random_forest', 'xgboost', 'lightgbm', 'catboost']:\n",
    "            if shuffle:\n",
    "                X_train, y_train = sklearn_shuffle(X_train, y_train, random_state=42)\n",
    "            self.model.fit(X_train, y_train)\n",
    "        elif self.model_type == 'cnnlstm':\n",
    "            X_train = X_train.reshape(-1, *self.input_shape)\n",
    "            # early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "            self.model.fit(X_train, y_train, epochs=epochs, verbose=False,shuffle=shuffle)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict using the chosen model.\"\"\"\n",
    "\n",
    "        if self.model_type == 'cnnlstm':\n",
    "            X = X.reshape(-1, *self.input_shape)\n",
    "            return self.model.predict(X, verbose=False)\n",
    "        else :\n",
    "            return self.model.predict(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6dab091-e5cb-4d89-b6df-c6b29d36b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(file, lag):\n",
    "    data = pd.read_csv(f\"{data_path}/{file}/{file}_INPUT.csv\")\n",
    "    scaler = StandardScaler()\n",
    "    sent_cols = sent_dict['NO_SENT']\n",
    "    data[sent_cols] = scaler.fit_transform(data[sent_cols])\n",
    "    data['ahead_Return'] = data['log_return'].shift(-1 * lag)\n",
    "    data['ahead_vol'] = data['Garchvol'].shift(-1 * lag)\n",
    "    data['ahead_mvol'] = data['MAvol'].shift(-1*(lag))\n",
    "    data = data[:-1 * lag]  # Drop rows corresponding to lag\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Create sequences of input data\n",
    "def sequences(X, y, timesteps):\n",
    "    \"\"\"\n",
    "    Generate sequences for time series models.\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    alpha, beta = [], []\n",
    "    n = timesteps\n",
    "    for i in range(X.shape[0]):\n",
    "        if i < n - 1:\n",
    "            continue\n",
    "        alpha.append(X[i - (n - 1):i + 1])\n",
    "        beta.append(y[i])\n",
    "\n",
    "    return np.asarray(alpha), np.asarray(beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81d22e89-e6f7-4e26-ad03-448313810971",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define helper functions\n",
    "def concat_results_and_mean(arrays):\n",
    "    \"\"\"\n",
    "    Concatenate and compute mean across all arrays.\n",
    "    \"\"\"\n",
    "    new_arrays = [np.array(single_arr).reshape(-1) for single_arr in arrays]\n",
    "    mean_array = np.mean(new_arrays, axis=0)\n",
    "    return mean_array\n",
    "\n",
    "def getanalysis(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate MAE and Directional Accuracy.\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    da = directional_accuracy(y_true, y_pred)\n",
    "    return mae, da\n",
    "\n",
    "def directional_accuracy(y_true, y_pred, mean_threshold=0):\n",
    "    \"\"\"\n",
    "    Calculate Directional Accuracy (DA) using the mean as a threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true : array-like\n",
    "        True target values scaled between 0 and 1.\n",
    "    y_pred : array-like\n",
    "        Predicted values scaled between 0 and 1.\n",
    "    mean_threshold : float\n",
    "        The mean value (threshold) used to decide the direction.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine the direction based on whether values are above or below the mean_threshold\n",
    "    correct_directions = (y_true >= mean_threshold) == (y_pred >= mean_threshold)\n",
    "    \n",
    "    # Return the mean of the correctly predicted directions\n",
    "    return np.mean(correct_directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8f94621-45ab-4b05-8f5d-120bd87444fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_analysis(results_list, actual_list):\n",
    "    \"\"\"\n",
    "    Calculate and return the average MAE and DA for a list of results.\n",
    "    \"\"\"\n",
    "    mae_list = []\n",
    "    da_list = []\n",
    "    \n",
    "    # Iterate over each segment of the results and actual values\n",
    "    for results, actual in zip(results_list, actual_list):\n",
    "        mae, da = getanalysis(actual, results)\n",
    "        mae_list.append(mae)\n",
    "        da_list.append(da)\n",
    "    \n",
    "    # Calculate the mean of MAE and DA\n",
    "    avg_mae = np.mean(mae_list)\n",
    "    avg_da = np.mean(da_list)\n",
    "    \n",
    "    return avg_mae, avg_da\n",
    "\n",
    "\n",
    "def analyze_etfs(etfs, predType, data_path):\n",
    "    for etf in etfs:\n",
    "        # Fetch the data\n",
    "        data = fetch(etf, 1)\n",
    "        \n",
    "        # Display basic info about the dataset\n",
    "        # print(f\"\\nBasic Info for {etf}:\\n\", data.info())\n",
    "\n",
    "        # Display descriptive statistics for the specified prediction type\n",
    "        print(f\"\\nDescriptive Statistics for '{predType}' column in {etf}:\\n\")\n",
    "        stats = pd.DataFrame(data[predType]).describe()\n",
    "        print(stats)\n",
    "\n",
    "        # Visualize distribution of the specified prediction type using a histogram\n",
    "        plt.figure(figsize=(7, 6))\n",
    "        sns.histplot(data[predType], kde=True, color='blue')\n",
    "        plt.title(f'Distribution of {predType} for {etf}')\n",
    "        plt.show()\n",
    "\n",
    "        # Visualize boxplot for the specified prediction type\n",
    "        plt.figure(figsize=(7, 6))\n",
    "        sns.boxplot(x=data[predType], color='blue')\n",
    "        plt.title(f'Boxplot for {predType} for {etf}')\n",
    "        plt.show()\n",
    "\n",
    "# Call the function\n",
    "# analyze_etfs(etfs, predType, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8824cfaf-ca5f-4670-9cb1-f31ae70074e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_train(model,X ,Y , train_window=500, test_window = 10,  pred_size=pred_size,sequence_size=sequence_size) :\n",
    "\n",
    "    result = []\n",
    "\n",
    "    # .reshape(X[:-pred_size].shape[0],1,sequence_size,-1)\n",
    "    model.fit(X[:-pred_size], Y[:-pred_size], epochs=100)\n",
    "\n",
    "    if model.model_type == 'cnnlstm' : \n",
    "        # tw_start and tw_end , means training window start index and end index\n",
    "        for tw_end in range(X.shape[0] - pred_size, X.shape[0], 10):\n",
    "            tw_start = tw_end - train_window\n",
    "    \n",
    "            X_train, y_train = X[tw_start:tw_end], Y[tw_start:tw_end]\n",
    "            # X_train = X_train.reshape(X_train.shape[0],1,sequence_size,-1)\n",
    "            model.fit(X_train, y_train, epochs=50 )\n",
    "    \n",
    "            test = X[tw_end:tw_end+test_window]  #.reshape(X[tw_end:tw_end+test_window].shape[0],1,sequence_size,-1)\n",
    "            preds = model.predict(test)\n",
    "            result.extend(preds)\n",
    "            print(tw_end, end=' ')\n",
    "    else :\n",
    "        result.extend(model.predict(X[-pred_size:]))\n",
    "\n",
    "    rmse_val = root_mean_squared_loss(Y[-pred_size:].reshape(-1), np.array(result).reshape(-1))\n",
    "\n",
    "    print(f\"model RMSE = {rmse_val}\")\n",
    "\n",
    "    return np.array(result), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf286399-6ea4-4e59-b2a2-0c1b290b84f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(etf, sequence_size, cross_window, model_type, sent_cols, no_sent_cols):\n",
    "    lag = 1\n",
    "    data = fetch(etf, lag) \n",
    "\n",
    "    SENT_X = data[sent_cols]\n",
    "    # Y = data[[predType]]  \n",
    "    NO_SENT_X = data[no_sent_cols]\n",
    "    \n",
    "    Y = np.array(data[[predType]])  \n",
    "\n",
    "    SENT_X, SENT_Y = sequences(SENT_X, Y, timesteps=sequence_size)\n",
    "    NO_SENT_X, NO_SENT_Y = sequences(NO_SENT_X, Y, timesteps=sequence_size)\n",
    "\n",
    "\n",
    "    sent_shape = SENT_X.shape[1:]\n",
    "    no_sent_shape = NO_SENT_X.shape[1:]\n",
    "\n",
    "\n",
    "\n",
    "        # machine learning regression models\n",
    "    SENT_X = SENT_X.reshape(SENT_X.shape[0], -1)\n",
    "    NO_SENT_X = NO_SENT_X.reshape(NO_SENT_X.shape[0], -1)\n",
    "    SENT_Y = SENT_Y.reshape(-1,)\n",
    "    NO_SENT_Y = NO_SENT_Y.reshape(-1,)\n",
    "\n",
    "\n",
    "    Y_PRED = Y[-pred_size:]  # Actual values for the prediction window\n",
    "    sent_predictions, no_sent_predictions = [], []\n",
    "    act_values = Y_PRED  # Actual values for this window\n",
    "\n",
    "    sent_model = model(model_type,sent_shape)\n",
    "    no_sent_model = model(model_type,no_sent_shape)\n",
    "    \n",
    "    # Perform cross validation over cross_window\n",
    "    for k in range(cross_window):\n",
    "\n",
    "\n",
    "        # Train and predict for SENT model\n",
    "        sent_pred, sent_model = sliding_window_train(sent_model,SENT_X, SENT_Y)\n",
    "        no_sent_pred, no_sent_model = sliding_window_train(no_sent_model,NO_SENT_X, NO_SENT_Y)\n",
    "        \n",
    "        sent_predictions.append(sent_pred)\n",
    "        no_sent_predictions.append(no_sent_pred)\n",
    "\n",
    "        mae_sent_temp, da_sent_temp = getanalysis(act_values, sent_pred)\n",
    "        mae_no_sent_temp, da_no_sent_temp = getanalysis(act_values, no_sent_pred)\n",
    "    \n",
    "        # Print metrics for the current ETF\n",
    "        print(etf , {\n",
    "            \"sent\": {\"mae\": mae_sent_temp, \"da\": da_sent_temp},\n",
    "            \"no_sent\": {\"mae\": mae_no_sent_temp, \"da\": da_no_sent_temp}\n",
    "        })\n",
    "        \n",
    "    mean_sent = (sent_predictions[-1]) \n",
    "    mean_no_sent = (no_sent_predictions[-1])  \n",
    "\n",
    "    # mean_sent = concat_results_and_mean(sent_predictions) \n",
    "    # mean_no_sent = concat_results_and_mean(no_sent_predictions)  \n",
    "\n",
    "    # Get prediction metrics for SENT and NO_SENT\n",
    "    mae_sent, da_sent = getanalysis(act_values, mean_sent)\n",
    "    mae_no_sent, da_no_sent = getanalysis(act_values, mean_no_sent)\n",
    "\n",
    "    # Print metrics for the current ETF\n",
    "    print(etf , {\n",
    "        \"sent\": {\"mae\": mae_sent, \"da\": da_sent},\n",
    "        \"no_sent\": {\"mae\": mae_no_sent, \"da\": da_no_sent}\n",
    "    })\n",
    "    \n",
    "    # Return the results for this ETF\n",
    "    return {\n",
    "        \"act\" : act_values,\n",
    "        \"sent\": {\"predictions\": mean_sent, \"mae\": mae_sent, \"da\": da_sent},\n",
    "        \"no_sent\": {\"predictions\": mean_no_sent, \"mae\": mae_no_sent, \"da\": da_no_sent}\n",
    "    }\n",
    "\n",
    "# Loop through ETFs and aggregate results\n",
    "def run_all_etfs(etfs, model_type, sequence_size=sequence_size, cross_window=2):\n",
    "    sent_results, no_sent_results = [], []\n",
    "    actual_array = []\n",
    "    \n",
    "    for etf in etfs:\n",
    "        print(f\"Training for ETF: {etf}\")\n",
    "        sent_cols = sent_dict['SENT']  # Columns for SENT model\n",
    "        no_sent_cols = sent_dict['NO_SENT']  # Columns for NO_SENT model\n",
    "\n",
    "        # Train and get results for the ETF\n",
    "        result = train(etf, sequence_size, cross_window, model_type, sent_cols, no_sent_cols)\n",
    "        \n",
    "        # Store the predictions and actual values\n",
    "        sent_results.append(result['sent']['predictions'])\n",
    "        no_sent_results.append(result['no_sent']['predictions'])\n",
    "        actual_array.append(result['act'])  # Collect actual values\n",
    "\n",
    "    # # Concatenate results across all ETFs\n",
    "    # sent_results = np.concatenate(sent_results, axis=0)\n",
    "    # no_sent_results = np.concatenate(no_sent_results, axis=0)\n",
    "    # actual_array = np.concatenate(actual_array, axis=0)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    # mae_sent, da_sent = getanalysis(actual_array, sent_results)\n",
    "    # mae_no_sent, da_no_sent = getanalysis(actual_array, no_sent_results)\n",
    "\n",
    "    mae_sent, da_sent = get_average_analysis(sent_results, actual_array)\n",
    "    mae_no_sent, da_no_sent = get_average_analysis(no_sent_results, actual_array)\n",
    "\n",
    "    # Print combined metrics\n",
    "    print(\"Combined results:\")\n",
    "    print(f\"SENT - MAE: {mae_sent}, DA: {da_sent}\")\n",
    "    print(f\"NO_SENT - MAE: {mae_no_sent}, DA: {da_no_sent}\")\n",
    "    \n",
    "    return sent_results, no_sent_results, actual_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "646d778c-3fb7-4060-ac7e-a7eec3cdf3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for ETF: ICLN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kali/.pyenv/versions/3.12.5/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1234 1244 1254 1264 1274 1284 1294 1304 1314 1324 1334 1344 1354 1364 1374 1384 1394 1404 1414 1424 1434 1444 1454 1464 1474 model RMSE = 1.1952449377186336\n",
      "1234 1244 1254 1264 1274 1284 1294 1304 1314 1324 1334 1344 1354 1364 1374 1384 1394 1404 1414 1424 1434 1444 1454 1464 1474 model RMSE = 1.2296449770836722\n",
      "ICLN {'sent': {'mae': 0.939481777236083, 'da': 0.516}, 'no_sent': {'mae': 0.9772583915767945, 'da': 0.464}}\n",
      "1234 1244 1254 1264 1274 1284 1294 1304 1314 1324 1334 1344 1354 1364 1374 1384 1394 1404 1414 1424 1434 1444 1454 1464 1474 model RMSE = 0.7075234213495364\n",
      "1234 1244 1254 1264 1274 1284 1294 1304 1314 1324 1334 1344 1354 1364 1374 1384 1394 1404 1414 1424 1434 1444 1454 1464 1474 model RMSE = 0.7584748766807894\n",
      "ICLN {'sent': {'mae': 0.5318439153552809, 'da': 0.696}, 'no_sent': {'mae': 0.5873516287923459, 'da': 0.66}}\n",
      "ICLN {'sent': {'mae': 0.5318439153552809, 'da': 0.696}, 'no_sent': {'mae': 0.5873516287923459, 'da': 0.66}}\n",
      "Training for ETF: PBD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kali/.pyenv/versions/3.12.5/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1234 1244 1254 1264 1274 1284 1294 1304 1314 1324 1334 1344 1354 1364 1374 1384 1394 1404 1414 1424 1434 1444 1454 1464 1474 model RMSE = 1.1803342753343649\n",
      "1234 1244 1254 1264 1274 1284 1294 1304 1314 1324 1334 1344 1354 1364 1374 1384 1394 1404 1414 1424 1434 1444 1454 1464 1474 model RMSE = 1.2053154470065726\n",
      "PBD {'sent': {'mae': 0.906427367641089, 'da': 0.456}, 'no_sent': {'mae': 0.9158126264320884, 'da': 0.44}}\n",
      "1234 1244 1254 1264 1274 1284 1294 1304 1314 1324 1334 1344 1354 1364 1374 1384 1394 1404 1414 1424 1434 1444 1454 1464 1474 model RMSE = 0.7127201031933279\n",
      "1234 1244 1254 1264 1274 1284 1294 1304 1314 1324 1334 1344 1354 1364 1374 1384 1394 1404 1414 1424 1434 1444 1454 1464 1474 model RMSE = 0.6954079001915061\n",
      "PBD {'sent': {'mae': 0.5402355773837219, 'da': 0.66}, 'no_sent': {'mae': 0.5374164572603279, 'da': 0.64}}\n",
      "PBD {'sent': {'mae': 0.5402355773837219, 'da': 0.66}, 'no_sent': {'mae': 0.5374164572603279, 'da': 0.64}}\n",
      "Training for ETF: QCLN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kali/.pyenv/versions/3.12.5/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1234 1244 1254 1264 1274 1284 1294 1304 1314 1324 1334 1344 1354 1364 1374 1384 1394 1404 1414 1424 1434 1444 1454 1464 1474 model RMSE = 1.3212397376923108\n",
      "1234 1244 1254 1264 1274 1284 1294 1304 1314 1324 1334 1344 1354 1364 1374 1384 1394 1404 1414 1424 1434 1444 1454 1464 1474 model RMSE = 1.613724613654404\n",
      "QCLN {'sent': {'mae': 1.0285278542090057, 'da': 0.492}, 'no_sent': {'mae': 1.2561915321310595, 'da': 0.436}}\n",
      "1234 1244 1254 1264 1274 1284 1294 1304 1314 1324 1334 1344 1354 1364 1374 1384 1394 1404 1414 1424 1434 1444 1454 1464 1474 model RMSE = 0.8089673635616994\n",
      "1234 1244 1254 1264 1274 1284 1294 1304 1314 1324 1334 1344 1354 1364 1374 1384 1394 1404 1414 1424 1434 1444 1454 1464 1474 model RMSE = 1.0032816418086221\n",
      "QCLN {'sent': {'mae': 0.6094576412913761, 'da': 0.68}, 'no_sent': {'mae': 0.7663494665334791, 'da': 0.62}}\n",
      "QCLN {'sent': {'mae': 0.6094576412913761, 'da': 0.68}, 'no_sent': {'mae': 0.7663494665334791, 'da': 0.62}}\n",
      "Combined results:\n",
      "SENT - MAE: 0.5605123780101263, DA: 0.6786666666666666\n",
      "NO_SENT - MAE: 0.6303725175287177, DA: 0.64\n"
     ]
    }
   ],
   "source": [
    "model_type = 'cnnlstm'\n",
    "\n",
    "sent_results, no_sent_results, actual_array = run_all_etfs(etfs, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9930aaa9-3828-4462-b665-53a916551ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8791db7-85c5-4172-bd1e-25badf8cd867",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
