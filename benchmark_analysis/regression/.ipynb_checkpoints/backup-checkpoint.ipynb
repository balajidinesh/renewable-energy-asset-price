{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "128e0c68-301d-4d0c-a90f-6951f31a70fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ebbbfe8-5f9c-4e6a-a74d-b825824a84e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 18:10:40.262355: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-24 18:10:40.274141: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-24 18:10:40.277996: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-24 18:10:40.286986: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-24 18:10:41.159734: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from scipy.special import expit\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle as sklearn_shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, LSTM, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# from misc_modules import dm_test, plot_double_standard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f37ca-5b1f-409c-b2af-0a375ad37898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72d41fe4-d030-459a-a604-695839bfc284",
   "metadata": {},
   "outputs": [],
   "source": [
    "etfs = ['ICLN', 'PBD', 'QCLN']  # ETF symbols\n",
    "sequence_size = 5  # Number of time steps in sequence\n",
    "cross_window = 3  # Number of cross-validation windows\n",
    "\n",
    "lags = [1]\n",
    "predType = 'ahead_Return'\n",
    "predLabel = 'Log-Return'\n",
    "\n",
    "pred_size=250\n",
    "model_name = f'reg-{predType}'\n",
    "\n",
    "save_path = '../../results'\n",
    "data_path = '../../data'\n",
    "# 'GT Sent', 'INV Sent', 'GT_VAL_SENT', 'INV_VAL_SENT',\n",
    "sent_dict =  {\n",
    "    'SENT': [ 'log_ovx', 'log_return', 'log_navR', 'GT Sent', 'INV Sent', 'd1-inv','d2-gt'],\n",
    "    'NO_SENT': [ 'log_ovx', 'log_return', 'log_navR']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0beab59-331f-4a68-aea8-56b34bbf5d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_loss(y_true,y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    return tf.keras.losses.Huber(delta=delta)(y_true, y_pred)\n",
    "\n",
    "def mean_squared_log_error(y_true, y_pred):\n",
    "    return tf.keras.losses.MeanSquaredLogarithmicError()(y_true, y_pred)\n",
    "\n",
    "\n",
    "class model:\n",
    "    def __init__(self, model_type, input_shape=None):\n",
    "        self.model_type = model_type\n",
    "        self.input_shape = input_shape\n",
    "        self.model = self._choose_model()\n",
    "\n",
    "    def _choose_model(self):\n",
    "        \"\"\"Choose and instantiate the model based on the provided model type.\"\"\"\n",
    "        if self.model_type == 'svr':\n",
    "            return SVR(kernel='rbf', C=1.0, gamma='scale')\n",
    "        elif self.model_type == 'random_forest':\n",
    "            return RandomForestRegressor(n_estimators=100, max_depth=None, min_samples_split=2, random_state=42)\n",
    "        elif self.model_type == 'xgboost':\n",
    "            return xgb.XGBRegressor(learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.8, random_state=42)\n",
    "        elif self.model_type == 'lightgbm':\n",
    "            return lgb.LGBMRegressor(num_leaves=31, learning_rate=0.01, n_estimators=100, bagging_fraction=0.8, random_state=42)\n",
    "        elif self.model_type == 'catboost':\n",
    "            return CatBoostRegressor(iterations=500, depth=6, learning_rate=0.1, verbose=0, random_state=42)\n",
    "        elif self.model_type == 'cnnlstm':\n",
    "            if self.input_shape is None:\n",
    "                raise ValueError(\"input_shape must be provided for cnnlstm model.\")\n",
    "            return self._build_cnn_lstm_model()\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid model type: {self.model_type}\")\n",
    "\n",
    "    def _build_cnn_lstm_model(self):\n",
    "        \"\"\"Build the CNN-LSTM model for time series or sequence data.\"\"\"\n",
    "        Timesteps, No_Features = self.input_shape\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(filters=32, kernel_size=1, activation='relu', input_shape=(Timesteps, No_Features)))\n",
    "        model.add(MaxPooling1D(pool_size=1))\n",
    "        model.add(LSTM(32, activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(8))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer='adam', loss=huber_loss, metrics=['mae'])\n",
    "        return model\n",
    "\n",
    "    def fit(self, X_train, y_train,epochs=100, batch_size=32, validation_split=0.2,shuffle=True):\n",
    "        \"\"\"Fit the model depending on the type.\"\"\"\n",
    "\n",
    "        \n",
    "        if self.model_type in ['svr', 'random_forest', 'xgboost', 'lightgbm', 'catboost']:\n",
    "            if shuffle:\n",
    "                X_train, y_train = sklearn_shuffle(X_train, y_train, random_state=42)\n",
    "            self.model.fit(X_train, y_train)\n",
    "        elif self.model_type == 'cnnlstm':\n",
    "            X_train = X_train.reshape(-1, *self.input_shape)\n",
    "            # early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "            self.model.fit(X_train, y_train, epochs=epochs, verbose=False,shuffle=shuffle)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict using the chosen model.\"\"\"\n",
    "        if self.model_type in ['svr', 'random_forest', 'xgboost', 'lightgbm', 'catboost']:\n",
    "            return self.model.predict(X)\n",
    "        elif self.model_type == 'cnnlstm':\n",
    "            X = X.reshape(-1, *self.input_shape)\n",
    "            return self.model.predict(X, verbose=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6dab091-e5cb-4d89-b6df-c6b29d36b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(file, lag):\n",
    "    data = pd.read_csv(f\"{data_path}/{file}/{file}_INPUT.csv\")\n",
    "    data['ahead_Return'] = data['log_return'].shift(-1 * lag)\n",
    "    data['ahead_vol'] = data['Garchvol'].shift(-1 * lag)\n",
    "    data['ahead_mvol'] = data['MAvol'].shift(-1*(lag))\n",
    "    data = data[:-1 * lag]  # Drop rows corresponding to lag\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    return data\n",
    "\n",
    "# Create sequences of input data\n",
    "def sequences(X, y, timesteps):\n",
    "    \"\"\"\n",
    "    Generate sequences for time series models.\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    alpha, beta = [], []\n",
    "    n = timesteps\n",
    "    for i in range(X.shape[0]):\n",
    "        if i < n - 1:\n",
    "            continue\n",
    "        alpha.append(X[i - (n - 1):i + 1])\n",
    "        beta.append(y[i])\n",
    "\n",
    "    return np.asarray(alpha), np.asarray(beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81d22e89-e6f7-4e26-ad03-448313810971",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define helper functions\n",
    "def concat_results_and_mean(arrays):\n",
    "    \"\"\"\n",
    "    Concatenate and compute mean across all arrays.\n",
    "    \"\"\"\n",
    "    new_arrays = [np.array(single_arr).reshape(-1) for single_arr in arrays]\n",
    "    mean_array = np.mean(new_arrays, axis=0)\n",
    "    return mean_array\n",
    "\n",
    "def getanalysis(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate MAE and Directional Accuracy.\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    da = directional_accuracy(y_true, y_pred)\n",
    "    return mae, da\n",
    "\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Directional Accuracy (DA).\n",
    "    \"\"\"\n",
    "\n",
    "    correct_directions = np.sign(y_true) == np.sign(y_pred)\n",
    "    # correct_directions = np.sign(y_true[1:] - y_true[:-1]) == np.sign(y_pred[1:] - y_pred[:-1])\n",
    "    \n",
    "    return np.mean(correct_directions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8f94621-45ab-4b05-8f5d-120bd87444fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_etfs(etfs, predType, data_path):\n",
    "    for etf in etfs:\n",
    "        # Fetch the data\n",
    "        data = fetch(etf, 1)\n",
    "        \n",
    "        # Display basic info about the dataset\n",
    "        # print(f\"\\nBasic Info for {etf}:\\n\", data.info())\n",
    "\n",
    "        # Display descriptive statistics for the specified prediction type\n",
    "        print(f\"\\nDescriptive Statistics for '{predType}' column in {etf}:\\n\")\n",
    "        stats = pd.DataFrame(data[predType]).describe()\n",
    "        print(stats)\n",
    "\n",
    "        # Visualize distribution of the specified prediction type using a histogram\n",
    "        plt.figure(figsize=(7, 6))\n",
    "        sns.histplot(data[predType], kde=True, color='blue')\n",
    "        plt.title(f'Distribution of {predType} for {etf}')\n",
    "        plt.show()\n",
    "\n",
    "        # Visualize boxplot for the specified prediction type\n",
    "        plt.figure(figsize=(7, 6))\n",
    "        sns.boxplot(x=data[predType], color='blue')\n",
    "        plt.title(f'Boxplot for {predType} for {etf}')\n",
    "        plt.show()\n",
    "\n",
    "# Call the function\n",
    "# analyze_etfs(etfs, predType, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8824cfaf-ca5f-4670-9cb1-f31ae70074e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_train(model,X ,Y , train_window=500, test_window = 10,  pred_size=pred_size,sequence_size=sequence_size) :\n",
    "\n",
    "    result = []\n",
    "\n",
    "    # .reshape(X[:-pred_size].shape[0],1,sequence_size,-1)\n",
    "    model.fit(X[:-pred_size], Y[:-pred_size], epochs=200)\n",
    "\n",
    "    if model.model_type == 'cnnlstm' : \n",
    "        # tw_start and tw_end , means training window start index and end index\n",
    "        for tw_end in range(X.shape[0] - pred_size, X.shape[0], 10):\n",
    "            tw_start = tw_end - train_window\n",
    "    \n",
    "            X_train, y_train = X[tw_start:tw_end], Y[tw_start:tw_end]\n",
    "            # X_train = X_train.reshape(X_train.shape[0],1,sequence_size,-1)\n",
    "            model.fit(X_train, y_train, epochs=50)\n",
    "    \n",
    "            test = X[tw_end:tw_end+test_window]  #.reshape(X[tw_end:tw_end+test_window].shape[0],1,sequence_size,-1)\n",
    "            preds = model.predict(test)\n",
    "            result.extend(preds)\n",
    "            print(tw_end, end=' ')\n",
    "    else :\n",
    "        result.extend(model.predict(X[-pred_size:]))\n",
    "\n",
    "    rmse_val = root_mean_squared_loss(Y[-pred_size:].reshape(-1), np.array(result).reshape(-1))\n",
    "\n",
    "    print(f\"model RMSE = {rmse_val}\")\n",
    "\n",
    "    return np.array(result), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf286399-6ea4-4e59-b2a2-0c1b290b84f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(etf, sequence_size, cross_window, model_type, sent_cols, no_sent_cols):\n",
    "    lag = 1\n",
    "    data = fetch(etf, lag) \n",
    "\n",
    "    SENT_X = data[sent_cols]\n",
    "    # Y = data[[predType]]  \n",
    "    NO_SENT_X = data[no_sent_cols]\n",
    "    \n",
    "    Y = np.array(data[[predType]])  \n",
    "\n",
    "    SENT_X, SENT_Y = sequences(SENT_X, Y, timesteps=sequence_size)\n",
    "    NO_SENT_X, NO_SENT_Y = sequences(NO_SENT_X, Y, timesteps=sequence_size)\n",
    "\n",
    "\n",
    "    sent_shape = SENT_X.shape[1:]\n",
    "    no_sent_shape = NO_SENT_X.shape[1:]\n",
    "\n",
    "    sent_model = model(model_type,sent_shape)\n",
    "    no_sent_model = model(model_type,no_sent_shape)\n",
    "\n",
    "        # machine learning regression models\n",
    "    SENT_X = SENT_X.reshape(SENT_X.shape[0], -1)\n",
    "    NO_SENT_X = NO_SENT_X.reshape(NO_SENT_X.shape[0], -1)\n",
    "    SENT_Y = SENT_Y.reshape(-1,)\n",
    "    NO_SENT_Y = NO_SENT_Y.reshape(-1,)\n",
    "\n",
    "\n",
    "    Y_PRED = Y[-pred_size:]  # Actual values for the prediction window\n",
    "    sent_predictions, no_sent_predictions = [], []\n",
    "    act_values = Y_PRED  # Actual values for this window\n",
    "    \n",
    "    # Perform cross validation over cross_window\n",
    "    for k in range(cross_window):\n",
    "\n",
    "        # Train and predict for SENT model\n",
    "        sent_pred, sent_model = sliding_window_train(sent_model,SENT_X, SENT_Y)\n",
    "        no_sent_pred, no_sent_model = sliding_window_train(no_sent_model,NO_SENT_X, NO_SENT_Y)\n",
    "        \n",
    "        sent_predictions.append(sent_pred)\n",
    "        no_sent_predictions.append(no_sent_pred)\n",
    "\n",
    "        mae_sent_temp, da_sent_temp = getanalysis(act_values, sent_pred)\n",
    "        mae_no_sent_temp, da_no_sent_temp = getanalysis(act_values, no_sent_pred)\n",
    "    \n",
    "        # Print metrics for the current ETF\n",
    "        print(etf , {\n",
    "            \"sent\": {\"mae\": mae_sent_temp, \"da\": da_sent_temp},\n",
    "            \"no_sent\": {\"mae\": mae_no_sent_temp, \"da\": da_no_sent_temp}\n",
    "        })\n",
    "    \n",
    "        mean_sent = (sent_predictions[-1]) \n",
    "        mean_no_sent = (no_sent_predictions[-1])  \n",
    "\n",
    "    # Get prediction metrics for SENT and NO_SENT\n",
    "    mae_sent, da_sent = getanalysis(act_values, mean_sent)\n",
    "    mae_no_sent, da_no_sent = getanalysis(act_values, mean_no_sent)\n",
    "\n",
    "    # Print metrics for the current ETF\n",
    "    print(etf , {\n",
    "        \"sent\": {\"mae\": mae_sent, \"da\": da_sent},\n",
    "        \"no_sent\": {\"mae\": mae_no_sent, \"da\": da_no_sent}\n",
    "    })\n",
    "    \n",
    "    # Return the results for this ETF\n",
    "    return {\n",
    "        \"act\" : act_values,\n",
    "        \"sent\": {\"predictions\": mean_sent, \"mae\": mae_sent, \"da\": da_sent},\n",
    "        \"no_sent\": {\"predictions\": mean_no_sent, \"mae\": mae_no_sent, \"da\": da_no_sent}\n",
    "    }\n",
    "\n",
    "# Loop through ETFs and aggregate results\n",
    "def run_all_etfs(etfs, model_type, sequence_size=sequence_size, cross_window=cross_window):\n",
    "    sent_results, no_sent_results = [], []\n",
    "    actual_array = []\n",
    "    \n",
    "    for etf in etfs:\n",
    "        print(f\"Training for ETF: {etf}\")\n",
    "        sent_cols = sent_dict['SENT']  # Columns for SENT model\n",
    "        no_sent_cols = sent_dict['NO_SENT']  # Columns for NO_SENT model\n",
    "\n",
    "        # Train and get results for the ETF\n",
    "        result = train(etf, sequence_size, cross_window, model_type, sent_cols, no_sent_cols)\n",
    "        \n",
    "        # Store the predictions and actual values\n",
    "        sent_results.append(result['sent']['predictions'])\n",
    "        no_sent_results.append(result['no_sent']['predictions'])\n",
    "        actual_array.append(result['act'])  # Collect actual values\n",
    "\n",
    "    # Concatenate results across all ETFs\n",
    "    sent_results = np.concatenate(sent_results, axis=0)\n",
    "    no_sent_results = np.concatenate(no_sent_results, axis=0)\n",
    "    actual_array = np.concatenate(actual_array, axis=0)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    mae_sent, da_sent = getanalysis(actual_array, sent_results)\n",
    "    mae_no_sent, da_no_sent = getanalysis(actual_array, no_sent_results)\n",
    "\n",
    "    # Print combined metrics\n",
    "    print(\"Combined results:\")\n",
    "    print(f\"SENT - MAE: {mae_sent}, DA: {da_sent}\")\n",
    "    print(f\"NO_SENT - MAE: {mae_no_sent}, DA: {da_no_sent}\")\n",
    "    \n",
    "    return sent_results, no_sent_results, actual_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "646d778c-3fb7-4060-ac7e-a7eec3cdf3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for ETF: ICLN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kali/.pyenv/versions/3.12.5/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-09-24 18:10:41.939959: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233 1243 1253 1263 1273 1283 1293 1303 1313 1323 1333 1343 1353 1363 1373 1383 1393 1403 1413 1423 1433 1443 1453 1463 1473 model RMSE = 0.014323566466674828\n",
      "1233 1243 1253 1263 1273 1283 1293 1303 1313 1323 1333 1343 1353 1363 1373 1383 1393 1403 1413 1423 1433 1443 1453 1463 1473 model RMSE = 0.012681585687461154\n",
      "ICLN {'sent': {'mae': 0.011281028677051679, 'da': 0.456}, 'no_sent': {'mae': 0.009219737613735459, 'da': 0.484}}\n",
      "1233 1243 1253 1263 1273 1283 1293 1303 1313 1323 1333 1343 1353 1363 1373 1383 1393 1403 1413 1423 1433 1443 1453 1463 1473 model RMSE = 0.012498179771217923\n",
      "1233 1243 1253 1263 1273 1283 1293 1303 1313 1323 1333 1343 1353 1363 1373 1383 1393 1403 1413 1423 1433 1443 1453 1463 1473 model RMSE = 0.013577573302722796\n",
      "ICLN {'sent': {'mae': 0.00990782992257968, 'da': 0.512}, 'no_sent': {'mae': 0.009992238171274205, 'da': 0.568}}\n",
      "1233 1243 1253 1263 1273 1283 1293 1303 1313 1323 1333 1343 1353 1363 1373 1383 1393 1403 1413 1423 1433 1443 1453 1463 1473 model RMSE = 0.010350892057231697\n",
      "1233 1243 1253 1263 1273 1283 1293 1303 1313 1323 1333 1343 1353 1363 1373 1383 1393 1403 1413 1423 1433 1443 1453 1463 1473 model RMSE = 0.012733156937450526\n",
      "ICLN {'sent': {'mae': 0.00813934646830429, 'da': 0.588}, 'no_sent': {'mae': 0.009242875873720812, 'da': 0.572}}\n",
      "ICLN {'sent': {'mae': 0.00813934646830429, 'da': 0.588}, 'no_sent': {'mae': 0.009242875873720812, 'da': 0.572}}\n",
      "Training for ETF: PBD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kali/.pyenv/versions/3.12.5/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233 1243 1253 1263 1273 1283 1293 1303 1313 1323 1333 1343 1353 1363 1373 1383 1393 1403 1413 1423 1433 1443 1453 1463 1473 model RMSE = 0.012951488426167009\n",
      "1233 1243 1253 1263 1273 1283 1293 1303 1313 1323 1333 1343 1353 1363 1373 1383 1393 1403 1413 1423 1433 1443 1453 1463 1473 model RMSE = 0.010959785555566284\n",
      "PBD {'sent': {'mae': 0.00992335083071756, 'da': 0.452}, 'no_sent': {'mae': 0.008489079281547346, 'da': 0.472}}\n",
      "1233 1243 1253 1263 1273 1283 1293 1303 1313 1323 1333 1343 1353 1363 1373 1383 1393 1403 1413 1423 1433 1443 1453 1463 1473 model RMSE = 0.011350221873430951\n",
      "1233 1243 1253 1263 1273 1283 1293 1303 1313 1323 1333 1343 1353 1363 1373 1383 1393 1403 1413 1423 1433 1443 1453 1463 1473 model RMSE = 0.01047723484838885\n",
      "PBD {'sent': {'mae': 0.008736870403851115, 'da': 0.564}, 'no_sent': {'mae': 0.007916124406060467, 'da': 0.544}}\n",
      "1233 1243 1253 1263 1273 1283 1293 1303 1313 1323 1333 1343 1353 1363 1373 1383 1393 1403 1413 1423 1433 1443 1453 1463 1473 model RMSE = 0.010426292999413337\n",
      "1233 1243 1253 1263 1273 1283 1293 1303 1313 1323 1333 1343 1353 1363 1373 1383 1393 1403 1413 1423 1433 1443 1453 1463 1473 model RMSE = 0.010581806103156295\n",
      "PBD {'sent': {'mae': 0.00777348985525408, 'da': 0.648}, 'no_sent': {'mae': 0.0081746883972475, 'da': 0.528}}\n",
      "PBD {'sent': {'mae': 0.00777348985525408, 'da': 0.648}, 'no_sent': {'mae': 0.0081746883972475, 'da': 0.528}}\n",
      "Training for ETF: QCLN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kali/.pyenv/versions/3.12.5/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233 1243 1253 1263 1273 1283 1293 1303 1313 1323 1333 1343 1353 1363 1373 1383 1393 1403 1413 1423 1433 1443 1453 1463 1473 model RMSE = 0.018388096649607864\n",
      "1233 1243 1253 1263 1273 1283 1293 1303 1313 1323 1333 1343 1353 1363 1373 1383 1393 1403 1413 1423 1433 1443 1453 1463 1473 model RMSE = 0.018265272495560588\n",
      "QCLN {'sent': {'mae': 0.0142866319766242, 'da': 0.424}, 'no_sent': {'mae': 0.013541103520262908, 'da': 0.496}}\n",
      "1233 1243 1253 1263 1273 1283 1293 1303 1313 1323 1333 1343 1353 1363 1373 1383 1393 1403 1413 1423 1433 1443 1453 1463 1473 model RMSE = 0.01584800675921326\n",
      "1233 1243 1253 1263 1273 1283 1293 1303 1313 1323 1333 1343 1353 1363 1373 1383 1393 1403 1413 1423 1433 1443 1453 1463 1473 model RMSE = 0.016762576348723596\n",
      "QCLN {'sent': {'mae': 0.011950078772029141, 'da': 0.58}, 'no_sent': {'mae': 0.012397871636908055, 'da': 0.596}}\n",
      "1233 1243 1253 1263 1273 1283 1293 1303 1313 1323 1333 1343 1353 1363 1373 1383 1393 1403 1413 1423 1433 1443 1453 1463 1473 model RMSE = 0.013828849026393834\n",
      "1233 1243 1253 1263 1273 1283 1293 1303 1313 1323 1333 1343 1353 1363 1373 1383 1393 1403 1413 1423 1433 1443 1453 1463 1473 model RMSE = 0.01645203869349416\n",
      "QCLN {'sent': {'mae': 0.009727348638874457, 'da': 0.676}, 'no_sent': {'mae': 0.01222709547277467, 'da': 0.62}}\n",
      "QCLN {'sent': {'mae': 0.009727348638874457, 'da': 0.676}, 'no_sent': {'mae': 0.01222709547277467, 'da': 0.62}}\n",
      "Combined results:\n",
      "SENT - MAE: 0.008546728320810942, DA: 0.6373333333333333\n",
      "NO_SENT - MAE: 0.009881553247914327, DA: 0.5733333333333334\n"
     ]
    }
   ],
   "source": [
    "model_type = 'cnnlstm'\n",
    "\n",
    "sent_results, no_sent_results, actual_array = run_all_etfs(etfs, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5893a023-859e-468d-8683-54ce183200e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for ETF: ICLN\n",
      "model RMSE = 0.009826896494937355\n",
      "model RMSE = 0.009826896494937355\n",
      "ICLN {'sent': {'mae': 0.007641349786716184, 'da': 0.42}, 'no_sent': {'mae': 0.007641349786716184, 'da': 0.42}}\n",
      "model RMSE = 0.009826896494937355\n",
      "model RMSE = 0.009826896494937355\n",
      "ICLN {'sent': {'mae': 0.007641349786716184, 'da': 0.42}, 'no_sent': {'mae': 0.007641349786716184, 'da': 0.42}}\n",
      "model RMSE = 0.009826896494937355\n",
      "model RMSE = 0.009826896494937355\n",
      "ICLN {'sent': {'mae': 0.007641349786716184, 'da': 0.42}, 'no_sent': {'mae': 0.007641349786716184, 'da': 0.42}}\n",
      "ICLN {'sent': {'mae': 0.007641349786716184, 'da': 0.42}, 'no_sent': {'mae': 0.007641349786716184, 'da': 0.42}}\n",
      "Training for ETF: PBD\n",
      "model RMSE = 0.009892896803594474\n",
      "model RMSE = 0.009892896803594474\n",
      "PBD {'sent': {'mae': 0.007975929651694157, 'da': 0.38}, 'no_sent': {'mae': 0.007975929651694157, 'da': 0.38}}\n",
      "model RMSE = 0.009892896803594474\n",
      "model RMSE = 0.009892896803594474\n",
      "PBD {'sent': {'mae': 0.007975929651694157, 'da': 0.38}, 'no_sent': {'mae': 0.007975929651694157, 'da': 0.38}}\n",
      "model RMSE = 0.009892896803594474\n",
      "model RMSE = 0.009892896803594474\n",
      "PBD {'sent': {'mae': 0.007975929651694157, 'da': 0.38}, 'no_sent': {'mae': 0.007975929651694157, 'da': 0.38}}\n",
      "PBD {'sent': {'mae': 0.007975929651694157, 'da': 0.38}, 'no_sent': {'mae': 0.007975929651694157, 'da': 0.38}}\n",
      "Training for ETF: QCLN\n",
      "model RMSE = 0.013169472617812938\n",
      "model RMSE = 0.013169472617812938\n",
      "QCLN {'sent': {'mae': 0.009724828778083911, 'da': 0.572}, 'no_sent': {'mae': 0.009724828778083911, 'da': 0.572}}\n",
      "model RMSE = 0.013169472617812938\n",
      "model RMSE = 0.013169472617812938\n",
      "QCLN {'sent': {'mae': 0.009724828778083911, 'da': 0.572}, 'no_sent': {'mae': 0.009724828778083911, 'da': 0.572}}\n",
      "model RMSE = 0.013169472617812938\n",
      "model RMSE = 0.013169472617812938\n",
      "QCLN {'sent': {'mae': 0.009724828778083911, 'da': 0.572}, 'no_sent': {'mae': 0.009724828778083911, 'da': 0.572}}\n",
      "QCLN {'sent': {'mae': 0.009724828778083911, 'da': 0.572}, 'no_sent': {'mae': 0.009724828778083911, 'da': 0.572}}\n",
      "Combined results:\n",
      "SENT - MAE: 0.008447369405498084, DA: 0.4537777777777778\n",
      "NO_SENT - MAE: 0.008447369405498084, DA: 0.4537777777777778\n"
     ]
    }
   ],
   "source": [
    "model_type = 'svr'\n",
    "\n",
    "sent_results, no_sent_results, actual_array = run_all_etfs(etfs, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61e7c826-0ee3-437e-b21a-8d625a23b34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for ETF: ICLN\n",
      "model RMSE = 0.009957530952751176\n",
      "model RMSE = 0.009814806126302275\n",
      "ICLN {'sent': {'mae': 0.007637142190159056, 'da': 0.478688}, 'no_sent': {'mae': 0.007496233111414595, 'da': 0.476896}}\n",
      "model RMSE = 0.009957530952751176\n",
      "model RMSE = 0.009814806126302275\n",
      "ICLN {'sent': {'mae': 0.007637142190159056, 'da': 0.478688}, 'no_sent': {'mae': 0.007496233111414595, 'da': 0.476896}}\n",
      "model RMSE = 0.009957530952751176\n",
      "model RMSE = 0.009814806126302275\n",
      "ICLN {'sent': {'mae': 0.007637142190159056, 'da': 0.478688}, 'no_sent': {'mae': 0.007496233111414595, 'da': 0.476896}}\n",
      "ICLN {'sent': {'mae': 0.007637142190159056, 'da': 0.478688}, 'no_sent': {'mae': 0.007496233111414595, 'da': 0.476896}}\n",
      "Training for ETF: PBD\n",
      "model RMSE = 0.008546313971301032\n",
      "model RMSE = 0.008602485480459601\n",
      "PBD {'sent': {'mae': 0.006516865701791441, 'da': 0.484}, 'no_sent': {'mae': 0.006549221999177294, 'da': 0.478176}}\n",
      "model RMSE = 0.008546313971301032\n",
      "model RMSE = 0.008602485480459601\n",
      "PBD {'sent': {'mae': 0.006516865701791441, 'da': 0.484}, 'no_sent': {'mae': 0.006549221999177294, 'da': 0.478176}}\n",
      "model RMSE = 0.008546313971301032\n",
      "model RMSE = 0.008602485480459601\n",
      "PBD {'sent': {'mae': 0.006516865701791441, 'da': 0.484}, 'no_sent': {'mae': 0.006549221999177294, 'da': 0.478176}}\n",
      "PBD {'sent': {'mae': 0.006516865701791441, 'da': 0.484}, 'no_sent': {'mae': 0.006549221999177294, 'da': 0.478176}}\n",
      "Training for ETF: QCLN\n",
      "model RMSE = 0.013430268043481372\n",
      "model RMSE = 0.013784529231539661\n",
      "QCLN {'sent': {'mae': 0.010262989679399677, 'da': 0.4664}, 'no_sent': {'mae': 0.010460345217385645, 'da': 0.467104}}\n",
      "model RMSE = 0.013430268043481372\n",
      "model RMSE = 0.013784529231539661\n",
      "QCLN {'sent': {'mae': 0.010262989679399677, 'da': 0.4664}, 'no_sent': {'mae': 0.010460345217385645, 'da': 0.467104}}\n",
      "model RMSE = 0.013430268043481372\n",
      "model RMSE = 0.013784529231539661\n",
      "QCLN {'sent': {'mae': 0.010262989679399677, 'da': 0.4664}, 'no_sent': {'mae': 0.010460345217385645, 'da': 0.467104}}\n",
      "QCLN {'sent': {'mae': 0.010262989679399677, 'da': 0.4664}, 'no_sent': {'mae': 0.010460345217385645, 'da': 0.467104}}\n",
      "Combined results:\n",
      "SENT - MAE: 0.00813899919045006, DA: 0.4771448888888889\n",
      "NO_SENT - MAE: 0.008168600109325845, DA: 0.47494044444444444\n"
     ]
    }
   ],
   "source": [
    "model_type = 'random_forest'\n",
    "\n",
    "sent_results, no_sent_results, actual_array = run_all_etfs(etfs, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0b982b9-0329-4402-ab97-7a72579139b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for ETF: ICLN\n",
      "model RMSE = 0.009643240647717176\n",
      "model RMSE = 0.00970742442426629\n",
      "ICLN {'sent': {'mae': 0.007465772474140169, 'da': 0.470624}, 'no_sent': {'mae': 0.007508645500159895, 'da': 0.4648}}\n",
      "model RMSE = 0.009643240647717176\n",
      "model RMSE = 0.00970742442426629\n",
      "ICLN {'sent': {'mae': 0.007465772474140169, 'da': 0.470624}, 'no_sent': {'mae': 0.007508645500159895, 'da': 0.4648}}\n",
      "model RMSE = 0.009643240647717176\n",
      "model RMSE = 0.00970742442426629\n",
      "ICLN {'sent': {'mae': 0.007465772474140169, 'da': 0.470624}, 'no_sent': {'mae': 0.007508645500159895, 'da': 0.4648}}\n",
      "ICLN {'sent': {'mae': 0.007465772474140169, 'da': 0.470624}, 'no_sent': {'mae': 0.007508645500159895, 'da': 0.4648}}\n",
      "Training for ETF: PBD\n",
      "model RMSE = 0.008461430503803843\n",
      "model RMSE = 0.008507158149044214\n",
      "PBD {'sent': {'mae': 0.006364578746798856, 'da': 0.480672}, 'no_sent': {'mae': 0.006396364957151655, 'da': 0.46736}}\n",
      "model RMSE = 0.008461430503803843\n",
      "model RMSE = 0.008507158149044214\n",
      "PBD {'sent': {'mae': 0.006364578746798856, 'da': 0.480672}, 'no_sent': {'mae': 0.006396364957151655, 'da': 0.46736}}\n",
      "model RMSE = 0.008461430503803843\n",
      "model RMSE = 0.008507158149044214\n",
      "PBD {'sent': {'mae': 0.006364578746798856, 'da': 0.480672}, 'no_sent': {'mae': 0.006396364957151655, 'da': 0.46736}}\n",
      "PBD {'sent': {'mae': 0.006364578746798856, 'da': 0.480672}, 'no_sent': {'mae': 0.006396364957151655, 'da': 0.46736}}\n",
      "Training for ETF: QCLN\n",
      "model RMSE = 0.013004899909781861\n",
      "model RMSE = 0.013072223048171997\n",
      "QCLN {'sent': {'mae': 0.009902625927123869, 'da': 0.467808}, 'no_sent': {'mae': 0.009922061550769357, 'da': 0.469216}}\n",
      "model RMSE = 0.013004899909781861\n",
      "model RMSE = 0.013072223048171997\n",
      "QCLN {'sent': {'mae': 0.009902625927123869, 'da': 0.467808}, 'no_sent': {'mae': 0.009922061550769357, 'da': 0.469216}}\n",
      "model RMSE = 0.013004899909781861\n",
      "model RMSE = 0.013072223048171997\n",
      "QCLN {'sent': {'mae': 0.009902625927123869, 'da': 0.467808}, 'no_sent': {'mae': 0.009922061550769357, 'da': 0.469216}}\n",
      "QCLN {'sent': {'mae': 0.009902625927123869, 'da': 0.467808}, 'no_sent': {'mae': 0.009922061550769357, 'da': 0.469216}}\n",
      "Combined results:\n",
      "SENT - MAE: 0.007910992382687632, DA: 0.472736\n",
      "NO_SENT - MAE: 0.007942357336026967, DA: 0.466784\n"
     ]
    }
   ],
   "source": [
    "model_type = 'xgboost'\n",
    "\n",
    "sent_results, no_sent_results, actual_array = run_all_etfs(etfs, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bcece44-fd78-437b-8515-d91e9f87a98a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for ETF: ICLN\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6395\n",
      "[LightGBM] [Info] Number of data points in the train set: 1233, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score -0.000190\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "model RMSE = 0.009802291813497755\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000122 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 1233, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score -0.000190\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "model RMSE = 0.009786478609854357\n",
      "ICLN {'sent': {'mae': 0.0075405494429502925, 'da': 0.475104}, 'no_sent': {'mae': 0.0075796632769853855, 'da': 0.474656}}\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000348 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6395\n",
      "[LightGBM] [Info] Number of data points in the train set: 1233, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score -0.000190\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "model RMSE = 0.009802291813497755\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 1233, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score -0.000190\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "model RMSE = 0.009786478609854357\n",
      "ICLN {'sent': {'mae': 0.0075405494429502925, 'da': 0.475104}, 'no_sent': {'mae': 0.0075796632769853855, 'da': 0.474656}}\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000312 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6395\n",
      "[LightGBM] [Info] Number of data points in the train set: 1233, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score -0.000190\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "model RMSE = 0.009802291813497755\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000151 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 1233, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score -0.000190\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "model RMSE = 0.009786478609854357\n",
      "ICLN {'sent': {'mae': 0.0075405494429502925, 'da': 0.475104}, 'no_sent': {'mae': 0.0075796632769853855, 'da': 0.474656}}\n",
      "ICLN {'sent': {'mae': 0.0075405494429502925, 'da': 0.475104}, 'no_sent': {'mae': 0.0075796632769853855, 'da': 0.474656}}\n",
      "Training for ETF: PBD\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000233 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6395\n",
      "[LightGBM] [Info] Number of data points in the train set: 1233, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score -0.000159\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "model RMSE = 0.008634043678368664\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000116 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 1233, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score -0.000159\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "model RMSE = 0.008681271838101265\n",
      "PBD {'sent': {'mae': 0.006550436204147189, 'da': 0.484}, 'no_sent': {'mae': 0.006533441423546281, 'da': 0.477344}}\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000228 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6395\n",
      "[LightGBM] [Info] Number of data points in the train set: 1233, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score -0.000159\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "model RMSE = 0.008634043678368664\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000111 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 1233, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score -0.000159\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "model RMSE = 0.008681271838101265\n",
      "PBD {'sent': {'mae': 0.006550436204147189, 'da': 0.484}, 'no_sent': {'mae': 0.006533441423546281, 'da': 0.477344}}\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000280 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6395\n",
      "[LightGBM] [Info] Number of data points in the train set: 1233, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score -0.000159\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "model RMSE = 0.008634043678368664\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000125 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 1233, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score -0.000159\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "model RMSE = 0.008681271838101265\n",
      "PBD {'sent': {'mae': 0.006550436204147189, 'da': 0.484}, 'no_sent': {'mae': 0.006533441423546281, 'da': 0.477344}}\n",
      "PBD {'sent': {'mae': 0.006550436204147189, 'da': 0.484}, 'no_sent': {'mae': 0.006533441423546281, 'da': 0.477344}}\n",
      "Training for ETF: QCLN\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000228 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6395\n",
      "[LightGBM] [Info] Number of data points in the train set: 1233, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score -0.000043\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "model RMSE = 0.012939378057293446\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000126 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 1233, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score -0.000043\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "model RMSE = 0.01317431307921618\n",
      "QCLN {'sent': {'mae': 0.009856063786203304, 'da': 0.48048}, 'no_sent': {'mae': 0.010057738601501182, 'da': 0.477664}}\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6395\n",
      "[LightGBM] [Info] Number of data points in the train set: 1233, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score -0.000043\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "model RMSE = 0.012939378057293446\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000118 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 1233, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score -0.000043\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "model RMSE = 0.01317431307921618\n",
      "QCLN {'sent': {'mae': 0.009856063786203304, 'da': 0.48048}, 'no_sent': {'mae': 0.010057738601501182, 'da': 0.477664}}\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6395\n",
      "[LightGBM] [Info] Number of data points in the train set: 1233, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score -0.000043\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "model RMSE = 0.012939378057293446\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000121 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 1233, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score -0.000043\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "model RMSE = 0.01317431307921618\n",
      "QCLN {'sent': {'mae': 0.009856063786203304, 'da': 0.48048}, 'no_sent': {'mae': 0.010057738601501182, 'da': 0.477664}}\n",
      "QCLN {'sent': {'mae': 0.009856063786203304, 'da': 0.48048}, 'no_sent': {'mae': 0.010057738601501182, 'da': 0.477664}}\n",
      "Combined results:\n",
      "SENT - MAE: 0.007982349811100261, DA: 0.47979022222222223\n",
      "NO_SENT - MAE: 0.00805694776734428, DA: 0.47692444444444443\n"
     ]
    }
   ],
   "source": [
    "model_type = 'lightgbm'\n",
    "\n",
    "sent_results, no_sent_results, actual_array = run_all_etfs(etfs, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebb3bd09-f662-4062-a9c0-d011020a9cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for ETF: ICLN\n",
      "model RMSE = 0.010350939047648932\n",
      "model RMSE = 0.01056450062434126\n",
      "ICLN {'sent': {'mae': 0.008054770336477541, 'da': 0.478688}, 'no_sent': {'mae': 0.008210034885370929, 'da': 0.474208}}\n",
      "model RMSE = 0.010350939047648932\n",
      "model RMSE = 0.01056450062434126\n",
      "ICLN {'sent': {'mae': 0.008054770336477541, 'da': 0.478688}, 'no_sent': {'mae': 0.008210034885370929, 'da': 0.474208}}\n",
      "model RMSE = 0.010350939047648932\n",
      "model RMSE = 0.01056450062434126\n",
      "ICLN {'sent': {'mae': 0.008054770336477541, 'da': 0.478688}, 'no_sent': {'mae': 0.008210034885370929, 'da': 0.474208}}\n",
      "ICLN {'sent': {'mae': 0.008054770336477541, 'da': 0.478688}, 'no_sent': {'mae': 0.008210034885370929, 'da': 0.474208}}\n",
      "Training for ETF: PBD\n",
      "model RMSE = 0.00880482914762377\n",
      "model RMSE = 0.008949021955244153\n",
      "PBD {'sent': {'mae': 0.006781033017919884, 'da': 0.49648}, 'no_sent': {'mae': 0.006844851492084243, 'da': 0.480672}}\n",
      "model RMSE = 0.00880482914762377\n",
      "model RMSE = 0.008949021955244153\n",
      "PBD {'sent': {'mae': 0.006781033017919884, 'da': 0.49648}, 'no_sent': {'mae': 0.006844851492084243, 'da': 0.480672}}\n",
      "model RMSE = 0.00880482914762377\n",
      "model RMSE = 0.008949021955244153\n",
      "PBD {'sent': {'mae': 0.006781033017919884, 'da': 0.49648}, 'no_sent': {'mae': 0.006844851492084243, 'da': 0.480672}}\n",
      "PBD {'sent': {'mae': 0.006781033017919884, 'da': 0.49648}, 'no_sent': {'mae': 0.006844851492084243, 'da': 0.480672}}\n",
      "Training for ETF: QCLN\n",
      "model RMSE = 0.013472460505085316\n",
      "model RMSE = 0.013720390858868047\n",
      "QCLN {'sent': {'mae': 0.010469782584673635, 'da': 0.475552}, 'no_sent': {'mae': 0.010629517614564255, 'da': 0.464288}}\n",
      "model RMSE = 0.013472460505085316\n",
      "model RMSE = 0.013720390858868047\n",
      "QCLN {'sent': {'mae': 0.010469782584673635, 'da': 0.475552}, 'no_sent': {'mae': 0.010629517614564255, 'da': 0.464288}}\n",
      "model RMSE = 0.013472460505085316\n",
      "model RMSE = 0.013720390858868047\n",
      "QCLN {'sent': {'mae': 0.010469782584673635, 'da': 0.475552}, 'no_sent': {'mae': 0.010629517614564255, 'da': 0.464288}}\n",
      "QCLN {'sent': {'mae': 0.010469782584673635, 'da': 0.475552}, 'no_sent': {'mae': 0.010629517614564255, 'da': 0.464288}}\n",
      "Combined results:\n",
      "SENT - MAE: 0.008435195313023686, DA: 0.4833173333333333\n",
      "NO_SENT - MAE: 0.00856146799733981, DA: 0.47339733333333334\n"
     ]
    }
   ],
   "source": [
    "model_type = 'catboost'\n",
    "\n",
    "sent_results, no_sent_results, actual_array = run_all_etfs(etfs, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5149179c-d59a-4455-9c95-97ff23f3234d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b826448-707d-417c-95b2-7fe0cd9f840e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
